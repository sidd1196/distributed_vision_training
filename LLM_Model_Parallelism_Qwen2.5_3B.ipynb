{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM Model Parallelism: From 1D to 5D Parallelism\n",
        "## Comprehensive Analysis with Qwen 2.5 3B\n",
        "\n",
        "This notebook demonstrates progressive model parallelism strategies for large language models, testing from 2 GPUs to 8 GPUs with comprehensive metrics tracking.\n",
        "\n",
        "**Model**: Qwen 2.5 3B (~3B parameters)  \n",
        "**Dataset**: MetaMathQA (395K math QA examples) - Industry-standard math reasoning dataset  \n",
        "**Frameworks**: DeepSpeed, PyTorch Native  \n",
        "**Progression**: DDP → 2D → 3D → 4D → 5D Parallelism\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Optional\n",
        "import json\n",
        "\n",
        "print(\"=== Environment Setup ===\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"  Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"\\n Environment check completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q transformers datasets accelerate deepspeed wandb sentencepiece protobuf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup WandB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "\n",
        "# Get the secret value\n",
        "user_secrets = UserSecretsClient()\n",
        "wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n",
        "\n",
        "# Log in using the key\n",
        "wandb.login(key=wandb_api_key)\n",
        "\n",
        "print(\" Successfully logged into W&B!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions: Metrics and MFU Calculation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Comprehensive metrics tracking and MFU calculation for LLM training.\n",
        "\"\"\"\n",
        "\n",
        "def calculate_llama_flops_per_token(config, seq_len=512):\n",
        "    \"\"\"\n",
        "    Calculate FLOPs for transformer forward+backward pass.\n",
        "    Based on Qwen/LLaMA architecture.\n",
        "    \"\"\"\n",
        "    num_layers = config.num_hidden_layers\n",
        "    hidden_size = config.hidden_size\n",
        "    intermediate_size = config.intermediate_size\n",
        "    num_heads = config.num_attention_heads\n",
        "    vocab_size = config.vocab_size\n",
        "    \n",
        "    # Embedding FLOPs (approximate)\n",
        "    embedding_flops = seq_len * hidden_size * 2\n",
        "    \n",
        "    # Per-layer FLOPs\n",
        "    # Attention: QKV projection + attention computation + output projection\n",
        "    attention_flops = (\n",
        "        3 * seq_len * hidden_size * hidden_size +  # QKV projection\n",
        "        2 * seq_len * seq_len * hidden_size +      # Attention computation\n",
        "        seq_len * hidden_size * hidden_size        # Output projection\n",
        "    )\n",
        "    \n",
        "    # MLP (Gated MLP: gate + up + down for Qwen)\n",
        "    mlp_flops = (\n",
        "        2 * seq_len * hidden_size * intermediate_size +  # Gate + Up\n",
        "        seq_len * intermediate_size * hidden_size          # Down\n",
        "    )\n",
        "    \n",
        "    # LayerNorm (2 per layer)\n",
        "    layernorm_flops = 4 * seq_len * hidden_size\n",
        "    \n",
        "    layer_flops = attention_flops + mlp_flops + layernorm_flops\n",
        "    \n",
        "    # Total forward FLOPs\n",
        "    forward_flops = embedding_flops + num_layers * layer_flops\n",
        "    \n",
        "    # LM Head\n",
        "    lm_head_flops = seq_len * hidden_size * vocab_size\n",
        "    \n",
        "    # Forward + Backward (backward ~2x forward)\n",
        "    total_flops = (forward_flops + lm_head_flops) * 3\n",
        "    \n",
        "    return total_flops\n",
        "\n",
        "def get_theoretical_flops_per_second(num_gpus=2, gpu_model=\"T4\"):\n",
        "    \"\"\"Get theoretical peak FLOPs per second.\"\"\"\n",
        "    gpu_tflops = {\n",
        "        \"T4\": 65,\n",
        "        \"V100\": 125,\n",
        "        \"A100\": 312,\n",
        "        \"A6000\": 150,\n",
        "        \"RTX3090\": 142,\n",
        "        \"RTX4090\": 330\n",
        "    }\n",
        "    \n",
        "    single_gpu_tflops = gpu_tflops.get(gpu_model, 65)\n",
        "    total_tflops = single_gpu_tflops * num_gpus\n",
        "    return total_tflops * 1e12  # Convert to FLOPS\n",
        "\n",
        "def calculate_mfu(model, config, batch_size, seq_len, step_time, num_gpus=2, gpu_model=\"T4\"):\n",
        "    \"\"\"Calculate Model FLOPs Utilization (MFU).\"\"\"\n",
        "    flops_per_token = calculate_llama_flops_per_token(config, seq_len)\n",
        "    total_flops = flops_per_token * batch_size * seq_len\n",
        "    \n",
        "    actual_flops_per_sec = total_flops / step_time\n",
        "    theoretical_flops_per_sec = get_theoretical_flops_per_second(num_gpus, gpu_model)\n",
        "    \n",
        "    mfu = (actual_flops_per_sec / theoretical_flops_per_sec) * 100\n",
        "    \n",
        "    return mfu, actual_flops_per_sec, theoretical_flops_per_sec\n",
        "\n",
        "class MetricsTracker:\n",
        "    \"\"\"Comprehensive metrics tracking for distributed training.\"\"\"\n",
        "    \n",
        "    def __init__(self, rank=0):\n",
        "        self.rank = rank\n",
        "        self.metrics = {\n",
        "            \"step_times\": [],\n",
        "            \"forward_times\": [],\n",
        "            \"backward_times\": [],\n",
        "            \"optimizer_times\": [],\n",
        "            \"communication_times\": [],\n",
        "            \"memory_allocated\": [],\n",
        "            \"memory_reserved\": [],\n",
        "            \"memory_peak\": [],\n",
        "            \"losses\": [],\n",
        "            \"throughput_samples\": [],\n",
        "            \"throughput_tokens\": [],\n",
        "        }\n",
        "    \n",
        "    def record_step(self, step_metrics: Dict):\n",
        "        \"\"\"Record metrics for a single step.\"\"\"\n",
        "        if self.rank == 0:\n",
        "            for key, value in step_metrics.items():\n",
        "                if key in self.metrics:\n",
        "                    self.metrics[key].append(value)\n",
        "    \n",
        "    def get_summary(self) -> Dict:\n",
        "        \"\"\"Get summary statistics.\"\"\"\n",
        "        if self.rank != 0:\n",
        "            return {}\n",
        "        \n",
        "        summary = {}\n",
        "        for key, values in self.metrics.items():\n",
        "            if values:\n",
        "                summary[f\"{key}_mean\"] = sum(values) / len(values)\n",
        "                summary[f\"{key}_min\"] = min(values)\n",
        "                summary[f\"{key}_max\"] = max(values)\n",
        "        \n",
        "        return summary\n",
        "\n",
        "print(\" Metrics utilities loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle Setup: Copy metamathqa_utils.py from input dataset\n",
        "import shutil\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Source path in Kaggle input dataset\n",
        "kaggle_input_path = \"/kaggle/input/mathqa-utils/metamathqa_utils.py\"\n",
        "working_dir_path = \"./metamathqa_utils.py\"\n",
        "\n",
        "# Copy file from Kaggle input to working directory\n",
        "if os.path.exists(kaggle_input_path):\n",
        "    shutil.copy(kaggle_input_path, working_dir_path)\n",
        "    print(f\"Copied metamathqa_utils.py from Kaggle input to working directory\")\n",
        "elif os.path.exists(working_dir_path):\n",
        "    print(\"Using existing metamathqa_utils.py in working directory\")\n",
        "else:\n",
        "    # Fallback: add Kaggle input path to sys.path\n",
        "    kaggle_utils_dir = \"/kaggle/input/mathqa-utils\"\n",
        "    if os.path.exists(kaggle_utils_dir):\n",
        "        sys.path.insert(0, kaggle_utils_dir)\n",
        "        print(f\"Added {kaggle_utils_dir} to Python path\")\n",
        "    else:\n",
        "        print(\"ERROR: metamathqa_utils.py not found. Please check dataset attachment.\")\n",
        "\n",
        "# Test the utility module\n",
        "from metamathqa_utils import load_metamathqa_dataset\n",
        "\n",
        "# Load dataset (small subset for testing)\n",
        "print(\"Testing MetaMathQA dataset loading...\")\n",
        "tokenized_dataset, tokenizer = load_metamathqa_dataset(split=\"train[:100]\", rank=0)\n",
        "\n",
        "print(f\"\\nDataset loaded successfully!\")\n",
        "print(f\"Number of examples: {len(tokenized_dataset)}\")\n",
        "print(f\"Sample keys: {tokenized_dataset[0].keys()}\")\n",
        "print(f\"Sequence length: {len(tokenized_dataset[0]['input_ids'])}\")\n",
        "print(f\"\\nDataset Configuration:\")\n",
        "print(f\"  - Model: Qwen 2.5 3B-Instruct\")\n",
        "print(f\"  - Dataset: MetaMathQA (meta-math/MetaMathQA)\")\n",
        "print(f\"  - Format: Instruction-following (Qwen chat template)\")\n",
        "print(f\"  - Max sequence length: 2048 tokens\")\n",
        "print(f\"  - Full dataset size: ~395K examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: 1D Parallelism - Data Parallelism (DDP)\n",
        "## Baseline: Standard DistributedDataParallel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_ddp_qwen3b_metamath.py\n",
        "\"\"\"\n",
        "1D Parallelism: Data Parallelism (DDP)\n",
        "Baseline experiment with standard PyTorch DDP using MetaMathQA dataset.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_from_disk\n",
        "import warnings\n",
        "import wandb\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch.profiler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# MFU calculation functions\n",
        "def calculate_llama_flops_per_token(config, seq_len=512):\n",
        "    \"\"\"Calculate FLOPs for transformer forward+backward pass.\"\"\"\n",
        "    num_layers = config.num_hidden_layers\n",
        "    hidden_size = config.hidden_size\n",
        "    intermediate_size = config.intermediate_size\n",
        "    vocab_size = config.vocab_size\n",
        "    \n",
        "    embedding_flops = seq_len * hidden_size * 2\n",
        "    attention_flops = (\n",
        "        3 * seq_len * hidden_size * hidden_size +\n",
        "        2 * seq_len * seq_len * hidden_size +\n",
        "        seq_len * hidden_size * hidden_size\n",
        "    )\n",
        "    mlp_flops = (\n",
        "        2 * seq_len * hidden_size * intermediate_size +\n",
        "        seq_len * intermediate_size * hidden_size\n",
        "    )\n",
        "    layernorm_flops = 4 * seq_len * hidden_size\n",
        "    layer_flops = attention_flops + mlp_flops + layernorm_flops\n",
        "    forward_flops = embedding_flops + num_layers * layer_flops\n",
        "    lm_head_flops = seq_len * hidden_size * vocab_size\n",
        "    total_flops = (forward_flops + lm_head_flops) * 3\n",
        "    return total_flops\n",
        "\n",
        "def get_theoretical_flops_per_second(num_gpus=2, gpu_model=\"T4\"):\n",
        "    \"\"\"Get theoretical peak FLOPs per second.\"\"\"\n",
        "    gpu_tflops = {\"T4\": 65, \"V100\": 125, \"A100\": 312, \"A6000\": 150, \"RTX3090\": 142, \"RTX4090\": 330}\n",
        "    single_gpu_tflops = gpu_tflops.get(gpu_model, 65)\n",
        "    return single_gpu_tflops * num_gpus * 1e12\n",
        "\n",
        "def calculate_mfu(model, config, batch_size, seq_len, step_time, num_gpus, gpu_model):\n",
        "    \"\"\"Calculate Model FLOPs Utilization (MFU).\"\"\"\n",
        "    flops_per_token = calculate_llama_flops_per_token(config, seq_len)\n",
        "    total_flops = flops_per_token * batch_size * seq_len\n",
        "    actual_flops_per_sec = total_flops / step_time\n",
        "    theoretical_flops_per_sec = get_theoretical_flops_per_second(num_gpus, gpu_model)\n",
        "    mfu = (actual_flops_per_sec / theoretical_flops_per_sec) * 100\n",
        "    return mfu, actual_flops_per_sec, theoretical_flops_per_sec\n",
        "\n",
        "# Setup\n",
        "local_rank = int(os.environ['LOCAL_RANK'])\n",
        "rank = int(os.environ['RANK'])\n",
        "world_size = int(os.environ['WORLD_SIZE'])\n",
        "\n",
        "torch.cuda.set_device(local_rank)\n",
        "device = torch.device(f\"cuda:{local_rank}\")\n",
        "\n",
        "dist.init_process_group(backend='nccl')\n",
        "\n",
        "# WandB\n",
        "if rank == 0:\n",
        "    wandb.init(\n",
        "        project=\"LLM-Model-Parallelism-Qwen3B\",\n",
        "        name=f\"DDP-Qwen3B-MetaMathQA-{world_size}GPUs\",\n",
        "        config={\n",
        "            \"model\": \"Qwen2.5-3B\",\n",
        "            \"dataset\": \"MetaMathQA\",\n",
        "            \"parallelism\": \"1D-Data-Parallelism\",\n",
        "            \"framework\": \"PyTorch DDP\",\n",
        "            \"num_gpus\": world_size\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Load model\n",
        "if rank == 0:\n",
        "    print(\"Loading Qwen 2.5 3B model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "model = DDP(model, device_ids=[local_rank], find_unused_parameters=False)\n",
        "\n",
        "# Data setup - MetaMathQA using shared utility\n",
        "from metamathqa_utils import load_metamathqa_dataset\n",
        "\n",
        "tokenized_dataset, tokenizer = load_metamathqa_dataset(\n",
        "    split=\"train[:1000]\",  # Use subset for testing\n",
        "    cache_dir=\"./metamathqa_tokenized_data\",\n",
        "    rank=rank\n",
        ")\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"MetaMathQA dataset loaded: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "# DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "MICRO_BATCH_SIZE = 1  # Small batch for 3B model\n",
        "train_sampler = DistributedSampler(tokenized_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=MICRO_BATCH_SIZE,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=lambda x: {\n",
        "        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in x]),\n",
        "        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in x]),\n",
        "        'labels': torch.stack([torch.tensor(item['labels']) for item in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "scaler = GradScaler()\n",
        "\n",
        "# Profiler trace handler\n",
        "def trace_handler(prof):\n",
        "    if rank == 0:\n",
        "        trace_dir = \"./profiler_logs/llm_1d_ddp_trace\"\n",
        "        os.makedirs(trace_dir, exist_ok=True)\n",
        "        prof.export_chrome_trace(f\"{trace_dir}/rank{rank}_trace.json\")\n",
        "        print(f\" Profiler trace saved to {trace_dir}/rank{rank}_trace.json\")\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "config = model.module.config if hasattr(model, 'module') else model.config\n",
        "\n",
        "# Profiler schedule: wait=1, warmup=1, active=3 (capture 3 steps), repeat=1\n",
        "with torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=trace_handler,\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    \n",
        "    for epoch in range(1):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        \n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if i >= 20:  # Short run for testing\n",
        "                break\n",
        "            \n",
        "            step_start_time = time.time()\n",
        "            \n",
        "            # Label training step\n",
        "            with torch.profiler.record_function(f\"## Training Step {i} ##\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                \n",
        "                # Mask padding tokens in labels\n",
        "                labels[labels == tokenizer.pad_token_id] = -100\n",
        "                \n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Label forward pass\n",
        "                forward_start = time.time()\n",
        "                with torch.profiler.record_function(\"## Forward Pass ##\"):\n",
        "                    with autocast():\n",
        "                        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                        loss = outputs.loss\n",
        "                forward_time = time.time() - forward_start\n",
        "                \n",
        "                # Label backward pass\n",
        "                backward_start = time.time()\n",
        "                with torch.profiler.record_function(\"## Backward Pass ##\"):\n",
        "                    scaler.scale(loss).backward()\n",
        "                backward_time = time.time() - backward_start\n",
        "                \n",
        "                # Label optimizer step\n",
        "                optimizer_start = time.time()\n",
        "                with torch.profiler.record_function(\"## Optimizer Step ##\"):\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                optimizer_time = time.time() - optimizer_start\n",
        "            \n",
        "            step_time = time.time() - step_start_time\n",
        "            \n",
        "            # Step profiler (only for first few steps)\n",
        "            if i < 5:  # Profile first 5 steps (wait=1, warmup=1, active=3)\n",
        "                prof.step()\n",
        "        \n",
        "        # Calculate MFU\n",
        "        effective_batch = MICRO_BATCH_SIZE * world_size\n",
        "        seq_len = input_ids.size(1)\n",
        "        mfu, actual_flops, theoretical_flops = calculate_mfu(\n",
        "            model.module if hasattr(model, 'module') else model,\n",
        "            config,\n",
        "            effective_batch,\n",
        "            seq_len,\n",
        "            step_time,\n",
        "            num_gpus=world_size,\n",
        "            gpu_model=\"T4\"\n",
        "        )\n",
        "        \n",
        "        if rank == 0:\n",
        "            mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            mem_reserved = torch.cuda.memory_reserved() / 1e9\n",
        "            throughput_samples = effective_batch / step_time\n",
        "            throughput_tokens = effective_batch * seq_len / step_time\n",
        "            \n",
        "            wandb.log({\n",
        "                \"loss\": loss.item(),\n",
        "                \"mfu_percent\": mfu,\n",
        "                \"step_time_sec\": step_time,\n",
        "                \"forward_time_sec\": forward_time,\n",
        "                \"backward_time_sec\": backward_time,\n",
        "                \"optimizer_time_sec\": optimizer_time,\n",
        "                \"memory_allocated_gb\": mem_allocated,\n",
        "                \"memory_reserved_gb\": mem_reserved,\n",
        "                \"throughput_samples_per_sec\": throughput_samples,\n",
        "                \"throughput_tokens_per_sec\": throughput_tokens,\n",
        "                \"actual_tflops\": actual_flops / 1e12,\n",
        "                \"theoretical_tflops\": theoretical_flops / 1e12\n",
        "            })\n",
        "            \n",
        "            print(f\"Step {i}: Loss={loss.item():.4f} | MFU={mfu:.2f}% | Time={step_time:.2f}s | Mem={mem_allocated:.2f}GB\")\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.finish()\n",
        "dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run DDP experiment on 2 GPUs with MetaMathQA\n",
        "# !torchrun --nproc_per_node=2 train_ddp_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: 2D Parallelism - Data + Pipeline Parallelism\n",
        "## Combining Data Parallelism with Pipeline Parallelism\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_2d_pipeline_qwen3b_metamath.py\n",
        "\"\"\"\n",
        "2D Parallelism: Data Parallelism + Pipeline Parallelism\n",
        "Using DeepSpeed Pipeline Parallelism with MetaMathQA dataset.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import deepspeed\n",
        "import torch.distributed as dist\n",
        "from transformers import AutoModelForCausalLM\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from metamathqa_utils import load_metamathqa_dataset\n",
        "import warnings\n",
        "import wandb\n",
        "import os\n",
        "import time\n",
        "import torch.profiler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Setup\n",
        "local_rank = int(os.environ['LOCAL_RANK'])\n",
        "rank = int(os.environ['RANK'])\n",
        "world_size = int(os.environ['WORLD_SIZE'])\n",
        "\n",
        "torch.cuda.set_device(local_rank)\n",
        "device = torch.device(f\"cuda:{local_rank}\")\n",
        "\n",
        "# DeepSpeed config for 2D parallelism (Pipeline + Data)\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": 1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\"stage\": 0},  # No ZeRO for pure pipeline parallelism\n",
        "    \"pipeline\": {\n",
        "        \"stages\": world_size,  # Each GPU is a pipeline stage\n",
        "        \"partition\": \"type\"\n",
        "    },\n",
        "    \"wall_clock_breakdown\": True\n",
        "}\n",
        "\n",
        "# Initialize DeepSpeed\n",
        "deepspeed.init_distributed()\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.init(\n",
        "        project=\"LLM-Model-Parallelism-Qwen3B\",\n",
        "        name=f\"2D-Pipeline-Data-Qwen3B-MetaMathQA-{world_size}GPUs\",\n",
        "        config={\n",
        "            \"model\": \"Qwen2.5-3B\",\n",
        "            \"dataset\": \"MetaMathQA\",\n",
        "            \"parallelism\": \"2D-Pipeline-Data\",\n",
        "            \"framework\": \"DeepSpeed\",\n",
        "            \"num_gpus\": world_size,\n",
        "            \"pipeline_stages\": world_size\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Load model\n",
        "if rank == 0:\n",
        "    print(\"Loading Qwen 2.5 3B model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "# Initialize DeepSpeed with pipeline parallelism\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    model_parameters=model.parameters(),\n",
        "    config=deepspeed_config\n",
        ")\n",
        "\n",
        "# Data setup - MetaMathQA using shared utility\n",
        "tokenized_dataset, tokenizer = load_metamathqa_dataset(\n",
        "    split=\"train[:1000]\",\n",
        "    cache_dir=\"./metamathqa_tokenized_data\",\n",
        "    rank=rank\n",
        ")\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"MetaMathQA dataset loaded: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "MICRO_BATCH_SIZE = 1\n",
        "train_sampler = DistributedSampler(tokenized_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=MICRO_BATCH_SIZE,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=lambda x: {\n",
        "        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in x]),\n",
        "        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in x]),\n",
        "        'labels': torch.stack([torch.tensor(item['labels']) for item in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "# Profiler trace handler\n",
        "def trace_handler(prof):\n",
        "    if rank == 0:\n",
        "        trace_dir = \"./profiler_logs/llm_2d_pipeline_trace\"\n",
        "        os.makedirs(trace_dir, exist_ok=True)\n",
        "        prof.export_chrome_trace(f\"{trace_dir}/rank{rank}_trace.json\")\n",
        "        print(f\"Profiler trace saved to {trace_dir}/rank{rank}_trace.json\")\n",
        "\n",
        "# Training loop\n",
        "model_engine.train()\n",
        "config = model_engine.module.config if hasattr(model_engine, 'module') else model.config\n",
        "\n",
        "# Profiler schedule: wait=1, warmup=1, active=3 (capture 3 steps), repeat=1\n",
        "with torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=trace_handler,\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    \n",
        "    for epoch in range(1):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        \n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if i >= 20:\n",
        "                break\n",
        "            \n",
        "            step_start_time = time.time()\n",
        "            \n",
        "            # Label training step\n",
        "            with torch.profiler.record_function(f\"## Training Step {i} ##\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                \n",
        "                # Mask padding tokens in labels\n",
        "                labels[labels == tokenizer.pad_token_id] = -100\n",
        "                \n",
        "                # Label forward pass\n",
        "                forward_start = time.time()\n",
        "                with torch.profiler.record_function(\"## Forward Pass ##\"):\n",
        "                    outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                forward_time = time.time() - forward_start\n",
        "                \n",
        "                # Label backward pass\n",
        "                backward_start = time.time()\n",
        "                with torch.profiler.record_function(\"## Backward Pass ##\"):\n",
        "                    model_engine.backward(loss)\n",
        "                backward_time = time.time() - backward_start\n",
        "                \n",
        "                # Label optimizer step\n",
        "                optimizer_start = time.time()\n",
        "                with torch.profiler.record_function(\"## Optimizer Step ##\"):\n",
        "                    model_engine.step()\n",
        "                optimizer_time = time.time() - optimizer_start\n",
        "            \n",
        "            step_time = time.time() - step_start_time\n",
        "            \n",
        "            # Step profiler (only for first few steps)\n",
        "            if i < 5:  # Profile first 5 steps (wait=1, warmup=1, active=3)\n",
        "                prof.step()\n",
        "        \n",
        "        if rank == 0:\n",
        "            mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            effective_batch = MICRO_BATCH_SIZE * world_size\n",
        "            seq_len = input_ids.size(1)\n",
        "            throughput_samples = effective_batch / step_time\n",
        "            throughput_tokens = effective_batch * seq_len / step_time\n",
        "            \n",
        "            wandb.log({\n",
        "                \"loss\": loss.item(),\n",
        "                \"step_time_sec\": step_time,\n",
        "                \"forward_time_sec\": forward_time,\n",
        "                \"backward_time_sec\": backward_time,\n",
        "                \"optimizer_time_sec\": optimizer_time,\n",
        "                \"memory_allocated_gb\": mem_allocated,\n",
        "                \"throughput_samples_per_sec\": throughput_samples,\n",
        "                \"throughput_tokens_per_sec\": throughput_tokens\n",
        "            })\n",
        "            \n",
        "            print(f\"Step {i}: Loss={loss.item():.4f} | Time={step_time:.2f}s | Mem={mem_allocated:.2f}GB\")\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.finish()\n",
        "dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run 2D Pipeline experiment on 2 GPUs\n",
        "# !deepspeed --num_gpus=2 train_2d_pipeline_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: 3D Parallelism - Data + Pipeline + Tensor Parallelism\n",
        "## Full 3D Parallelism: The Standard for Large-Scale Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_3d_parallelism_qwen3b_metamath.py\n",
        "\"\"\"\n",
        "3D Parallelism: Data + Pipeline + Tensor Parallelism\n",
        "Combining all three parallelism strategies with MetaMathQA dataset.\n",
        "For 2 GPUs: 2 pipeline stages, 1-way tensor parallelism\n",
        "For 8 GPUs: 2 pipeline stages × 2-way tensor × 2 data parallel\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import deepspeed\n",
        "import torch.distributed as dist\n",
        "from transformers import AutoModelForCausalLM\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from metamathqa_utils import load_metamathqa_dataset\n",
        "import warnings\n",
        "import wandb\n",
        "import os\n",
        "import time\n",
        "import torch.profiler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Setup\n",
        "local_rank = int(os.environ['LOCAL_RANK'])\n",
        "rank = int(os.environ['RANK'])\n",
        "world_size = int(os.environ['WORLD_SIZE'])\n",
        "\n",
        "torch.cuda.set_device(local_rank)\n",
        "device = torch.device(f\"cuda:{local_rank}\")\n",
        "\n",
        "# 3D Parallelism Configuration\n",
        "if world_size == 2:\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    dp_size = 1\n",
        "elif world_size == 8:\n",
        "    pp_size = 2\n",
        "    tp_size = 2\n",
        "    dp_size = 2\n",
        "else:\n",
        "    pp_size = 2\n",
        "    tp_size = world_size // 4\n",
        "    dp_size = world_size // (pp_size * tp_size)\n",
        "\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": 1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\"stage\": 0},\n",
        "    \"pipeline\": {\"stages\": pp_size, \"partition\": \"type\"},\n",
        "    \"tensor_parallel\": {\"tp_size\": tp_size},\n",
        "    \"wall_clock_breakdown\": True\n",
        "}\n",
        "\n",
        "deepspeed.init_distributed()\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.init(\n",
        "        project=\"LLM-Model-Parallelism-Qwen3B\",\n",
        "        name=f\"3D-Parallelism-Qwen3B-MetaMathQA-{world_size}GPUs\",\n",
        "        config={\n",
        "            \"model\": \"Qwen2.5-3B\",\n",
        "            \"dataset\": \"MetaMathQA\",\n",
        "            \"parallelism\": \"3D-Data-Pipeline-Tensor\",\n",
        "            \"framework\": \"DeepSpeed\",\n",
        "            \"num_gpus\": world_size,\n",
        "            \"pipeline_stages\": pp_size,\n",
        "            \"tensor_parallel_size\": tp_size,\n",
        "            \"data_parallel_size\": dp_size\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Load model\n",
        "if rank == 0:\n",
        "    print(f\"Loading Qwen 2.5 3B with 3D parallelism: PP={pp_size}, TP={tp_size}, DP={dp_size}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    model_parameters=model.parameters(),\n",
        "    config=deepspeed_config\n",
        ")\n",
        "\n",
        "# Data setup - MetaMathQA using shared utility\n",
        "tokenized_dataset, tokenizer = load_metamathqa_dataset(\n",
        "    split=\"train[:1000]\",\n",
        "    cache_dir=\"./metamathqa_tokenized_data\",\n",
        "    rank=rank\n",
        ")\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"MetaMathQA dataset loaded: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "MICRO_BATCH_SIZE = 1\n",
        "train_sampler = DistributedSampler(tokenized_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=MICRO_BATCH_SIZE,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=lambda x: {\n",
        "        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in x]),\n",
        "        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in x]),\n",
        "        'labels': torch.stack([torch.tensor(item['labels']) for item in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "# Profiler trace handler\n",
        "def trace_handler(prof):\n",
        "    if rank == 0:\n",
        "        trace_dir = \"./profiler_logs/llm_3d_parallelism_trace\"\n",
        "        os.makedirs(trace_dir, exist_ok=True)\n",
        "        prof.export_chrome_trace(f\"{trace_dir}/rank{rank}_trace.json\")\n",
        "        print(f\"Profiler trace saved to {trace_dir}/rank{rank}_trace.json\")\n",
        "\n",
        "# Training loop\n",
        "model_engine.train()\n",
        "\n",
        "# Profiler schedule: wait=1, warmup=1, active=3 (capture 3 steps), repeat=1\n",
        "with torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=trace_handler,\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    \n",
        "    for epoch in range(1):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        \n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if i >= 20:\n",
        "                break\n",
        "            \n",
        "            step_start_time = time.time()\n",
        "            \n",
        "            # Label training step\n",
        "            with torch.profiler.record_function(f\"## Training Step {i} ##\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                \n",
        "                labels[labels == tokenizer.pad_token_id] = -100\n",
        "                \n",
        "                # Label forward pass\n",
        "                with torch.profiler.record_function(\"## Forward Pass ##\"):\n",
        "                    outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                \n",
        "                # Label backward pass\n",
        "                with torch.profiler.record_function(\"## Backward Pass ##\"):\n",
        "                    model_engine.backward(loss)\n",
        "                \n",
        "                # Label optimizer step\n",
        "                with torch.profiler.record_function(\"## Optimizer Step ##\"):\n",
        "                    model_engine.step()\n",
        "            \n",
        "            step_time = time.time() - step_start_time\n",
        "            \n",
        "            # Step profiler (only for first few steps)\n",
        "            if i < 5:  # Profile first 5 steps (wait=1, warmup=1, active=3)\n",
        "                prof.step()\n",
        "        \n",
        "        if rank == 0:\n",
        "            mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            effective_batch = MICRO_BATCH_SIZE * dp_size\n",
        "            seq_len = input_ids.size(1)\n",
        "            throughput_samples = effective_batch / step_time\n",
        "            throughput_tokens = effective_batch * seq_len / step_time\n",
        "            \n",
        "            wandb.log({\n",
        "                \"loss\": loss.item(),\n",
        "                \"step_time_sec\": step_time,\n",
        "                \"memory_allocated_gb\": mem_allocated,\n",
        "                \"throughput_samples_per_sec\": throughput_samples,\n",
        "                \"throughput_tokens_per_sec\": throughput_tokens\n",
        "            })\n",
        "            \n",
        "            print(f\"Step {i}: Loss={loss.item():.4f} | Time={step_time:.2f}s | Mem={mem_allocated:.2f}GB\")\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.finish()\n",
        "dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run 3D Parallelism experiment on 2 GPUs\n",
        "# !deepspeed --num_gpus=2 train_3d_parallelism_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: 4D Parallelism - Adding Sequence Parallelism\n",
        "## Data + Pipeline + Tensor + Sequence Parallelism\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_4d_parallelism_qwen3b_metamath.py\n",
        "\"\"\"\n",
        "4D Parallelism: Data + Pipeline + Tensor + Sequence Parallelism\n",
        "Sequence parallelism splits the sequence dimension across GPUs.\n",
        "This is particularly useful for long sequences in MetaMathQA.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import deepspeed\n",
        "import torch.distributed as dist\n",
        "from transformers import AutoModelForCausalLM\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from metamathqa_utils import load_metamathqa_dataset\n",
        "import warnings\n",
        "import wandb\n",
        "import os\n",
        "import time\n",
        "import torch.profiler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Setup\n",
        "local_rank = int(os.environ['LOCAL_RANK'])\n",
        "rank = int(os.environ['RANK'])\n",
        "world_size = int(os.environ['WORLD_SIZE'])\n",
        "\n",
        "torch.cuda.set_device(local_rank)\n",
        "device = torch.device(f\"cuda:{local_rank}\")\n",
        "\n",
        "# 4D Parallelism Configuration\n",
        "if world_size == 2:\n",
        "    dp_size = 1\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    sp_size = 1\n",
        "elif world_size == 8:\n",
        "    dp_size = 2\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    sp_size = 2\n",
        "else:\n",
        "    dp_size = 1\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    sp_size = world_size // (dp_size * pp_size * tp_size)\n",
        "\n",
        "deepspeed.init_distributed()\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.init(\n",
        "        project=\"LLM-Model-Parallelism-Qwen3B\",\n",
        "        name=f\"4D-Parallelism-Qwen3B-MetaMathQA-{world_size}GPUs\",\n",
        "        config={\n",
        "            \"model\": \"Qwen2.5-3B\",\n",
        "            \"dataset\": \"MetaMathQA\",\n",
        "            \"parallelism\": \"4D-Data-Pipeline-Tensor-Sequence\",\n",
        "            \"framework\": \"DeepSpeed + Custom SP\",\n",
        "            \"num_gpus\": world_size,\n",
        "            \"data_parallel_size\": dp_size,\n",
        "            \"pipeline_stages\": pp_size,\n",
        "            \"tensor_parallel_size\": tp_size,\n",
        "            \"sequence_parallel_size\": sp_size\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Custom Sequence Parallel Attention Layer (conceptual)\n",
        "class SequenceParallelAttention(nn.Module):\n",
        "    \"\"\"Attention layer with sequence parallelism.\"\"\"\n",
        "    def __init__(self, config, rank, world_size, sp_size):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.sp_size = sp_size\n",
        "        self.num_heads = config.num_attention_heads\n",
        "        self.head_dim = config.hidden_size // config.num_attention_heads\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.qkv = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=False)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        \n",
        "    def forward(self, hidden_states):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
        "        seq_len_per_rank = seq_len // self.sp_size\n",
        "        start_idx = (self.rank % self.sp_size) * seq_len_per_rank\n",
        "        end_idx = start_idx + seq_len_per_rank\n",
        "        \n",
        "        local_hidden = hidden_states[:, start_idx:end_idx, :]\n",
        "        qkv = self.qkv(local_hidden)\n",
        "        qkv = qkv.reshape(batch_size, seq_len_per_rank, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        \n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_output = torch.matmul(attn_weights, v)\n",
        "        \n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
        "        attn_output = attn_output.reshape(batch_size, seq_len_per_rank, hidden_size)\n",
        "        output = self.out_proj(attn_output)\n",
        "        \n",
        "        gathered_outputs = [torch.zeros_like(output) for _ in range(self.sp_size)]\n",
        "        dist.all_gather(gathered_outputs, output)\n",
        "        final_output = torch.cat(gathered_outputs, dim=1)\n",
        "        \n",
        "        return final_output\n",
        "\n",
        "# Load model\n",
        "if rank == 0:\n",
        "    print(f\"Loading Qwen 2.5 3B with 4D parallelism: DP={dp_size}, PP={pp_size}, TP={tp_size}, SP={sp_size}\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "# Note: Full sequence parallelism integration would require modifying model architecture\n",
        "# This is a conceptual demonstration\n",
        "\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": 1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\"stage\": 0},\n",
        "    \"pipeline\": {\"stages\": pp_size},\n",
        "    \"tensor_parallel\": {\"tp_size\": tp_size},\n",
        "    \"wall_clock_breakdown\": True\n",
        "}\n",
        "\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    model_parameters=model.parameters(),\n",
        "    config=deepspeed_config\n",
        ")\n",
        "\n",
        "# Data setup - MetaMathQA using shared utility\n",
        "tokenized_dataset, tokenizer = load_metamathqa_dataset(\n",
        "    split=\"train[:1000]\",\n",
        "    cache_dir=\"./metamathqa_tokenized_data\",\n",
        "    rank=rank\n",
        ")\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"MetaMathQA dataset loaded: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "MICRO_BATCH_SIZE = 1\n",
        "train_sampler = DistributedSampler(tokenized_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=MICRO_BATCH_SIZE,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=lambda x: {\n",
        "        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in x]),\n",
        "        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in x]),\n",
        "        'labels': torch.stack([torch.tensor(item['labels']) for item in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "# Profiler trace handler\n",
        "def trace_handler(prof):\n",
        "    if rank == 0:\n",
        "        trace_dir = \"./profiler_logs/llm_4d_parallelism_trace\"\n",
        "        os.makedirs(trace_dir, exist_ok=True)\n",
        "        prof.export_chrome_trace(f\"{trace_dir}/rank{rank}_trace.json\")\n",
        "        print(f\"Profiler trace saved to {trace_dir}/rank{rank}_trace.json\")\n",
        "\n",
        "# Training loop\n",
        "model_engine.train()\n",
        "\n",
        "# Profiler schedule: wait=1, warmup=1, active=3 (capture 3 steps), repeat=1\n",
        "with torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=trace_handler,\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    \n",
        "    for epoch in range(1):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        \n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if i >= 20:\n",
        "                break\n",
        "            \n",
        "            step_start_time = time.time()\n",
        "            \n",
        "            # Label training step\n",
        "            with torch.profiler.record_function(f\"## Training Step {i} ##\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                \n",
        "                labels[labels == tokenizer.pad_token_id] = -100\n",
        "                \n",
        "                # Label forward pass\n",
        "                with torch.profiler.record_function(\"## Forward Pass ##\"):\n",
        "                    outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                \n",
        "                # Label backward pass\n",
        "                with torch.profiler.record_function(\"## Backward Pass ##\"):\n",
        "                    model_engine.backward(loss)\n",
        "                \n",
        "                # Label optimizer step\n",
        "                with torch.profiler.record_function(\"## Optimizer Step ##\"):\n",
        "                    model_engine.step()\n",
        "            \n",
        "            step_time = time.time() - step_start_time\n",
        "            \n",
        "            # Step profiler (only for first few steps)\n",
        "            if i < 5:  # Profile first 5 steps (wait=1, warmup=1, active=3)\n",
        "                prof.step()\n",
        "        \n",
        "        if rank == 0:\n",
        "            mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            effective_batch = MICRO_BATCH_SIZE * dp_size\n",
        "            seq_len = input_ids.size(1)\n",
        "            throughput_samples = effective_batch / step_time\n",
        "            throughput_tokens = effective_batch * seq_len / step_time\n",
        "            \n",
        "            wandb.log({\n",
        "                \"loss\": loss.item(),\n",
        "                \"step_time_sec\": step_time,\n",
        "                \"memory_allocated_gb\": mem_allocated,\n",
        "                \"throughput_samples_per_sec\": throughput_samples,\n",
        "                \"throughput_tokens_per_sec\": throughput_tokens\n",
        "            })\n",
        "            \n",
        "            print(f\"Step {i}: Loss={loss.item():.4f} | Time={step_time:.2f}s | Mem={mem_allocated:.2f}GB\")\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.finish()\n",
        "dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run 4D Parallelism experiment on 2 GPUs\n",
        "# !deepspeed --num_gpus=2 train_4d_parallelism_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 5: 5D Parallelism - Adding Expert Parallelism (MoE)\n",
        "## Full 5D Parallelism: Data + Pipeline + Tensor + Sequence + Expert Parallelism\n",
        "## For Mixture-of-Experts Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile train_5d_parallelism_qwen3b_metamath.py\n",
        "\"\"\"\n",
        "5D Parallelism: Data + Pipeline + Tensor + Sequence + Expert Parallelism\n",
        "Full 5D parallelism for Mixture-of-Experts (MoE) models with MetaMathQA dataset.\n",
        "This demonstrates the complete parallelism framework.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import deepspeed\n",
        "import torch.distributed as dist\n",
        "from transformers import AutoModelForCausalLM\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "from metamathqa_utils import load_metamathqa_dataset\n",
        "import warnings\n",
        "import wandb\n",
        "import os\n",
        "import time\n",
        "import torch.profiler\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Setup\n",
        "local_rank = int(os.environ['LOCAL_RANK'])\n",
        "rank = int(os.environ['RANK'])\n",
        "world_size = int(os.environ['WORLD_SIZE'])\n",
        "\n",
        "torch.cuda.set_device(local_rank)\n",
        "device = torch.device(f\"cuda:{local_rank}\")\n",
        "\n",
        "# 5D Parallelism Configuration\n",
        "if world_size == 2:\n",
        "    dp_size = 1\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    sp_size = 1\n",
        "    ep_size = 1\n",
        "elif world_size == 8:\n",
        "    dp_size = 1\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    sp_size = 1\n",
        "    ep_size = 4\n",
        "else:\n",
        "    dp_size = 1\n",
        "    pp_size = 2\n",
        "    tp_size = 1\n",
        "    sp_size = 1\n",
        "    ep_size = world_size // (dp_size * pp_size * tp_size * sp_size)\n",
        "\n",
        "deepspeed.init_distributed()\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.init(\n",
        "        project=\"LLM-Model-Parallelism-Qwen3B\",\n",
        "        name=f\"5D-Parallelism-Qwen3B-MetaMathQA-{world_size}GPUs\",\n",
        "        config={\n",
        "            \"model\": \"Qwen2.5-3B\",\n",
        "            \"dataset\": \"MetaMathQA\",\n",
        "            \"parallelism\": \"5D-Full-Parallelism\",\n",
        "            \"framework\": \"DeepSpeed + Custom MoE\",\n",
        "            \"num_gpus\": world_size,\n",
        "            \"data_parallel_size\": dp_size,\n",
        "            \"pipeline_stages\": pp_size,\n",
        "            \"tensor_parallel_size\": tp_size,\n",
        "            \"sequence_parallel_size\": sp_size,\n",
        "            \"expert_parallel_size\": ep_size\n",
        "        }\n",
        "    )\n",
        "\n",
        "# Mixture-of-Experts Layer with Expert Parallelism\n",
        "class MoELayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Mixture-of-Experts layer demonstrating expert parallelism.\n",
        "    Experts are distributed across GPUs.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_experts=4, expert_capacity=2, rank=0, world_size=1, ep_size=1):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.expert_capacity = expert_capacity\n",
        "        self.rank = rank\n",
        "        self.world_size = world_size\n",
        "        self.ep_size = ep_size\n",
        "        \n",
        "        experts_per_gpu = num_experts // ep_size\n",
        "        expert_start = (rank % ep_size) * experts_per_gpu\n",
        "        expert_end = expert_start + experts_per_gpu\n",
        "        \n",
        "        self.local_experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_size, hidden_size * 2),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_size * 2, hidden_size)\n",
        "            ) for _ in range(experts_per_gpu)\n",
        "        ])\n",
        "        \n",
        "        self.local_expert_indices = list(range(expert_start, expert_end))\n",
        "        self.gate = nn.Linear(hidden_size, num_experts)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, hidden_size = x.shape\n",
        "        \n",
        "        gate_logits = self.gate(x)\n",
        "        gate_probs = F.softmax(gate_logits, dim=-1)\n",
        "        \n",
        "        top_k = 2\n",
        "        top_k_gate_probs, top_k_indices = torch.topk(gate_probs, k=top_k, dim=-1)\n",
        "        top_k_gate_probs = top_k_gate_probs / top_k_gate_probs.sum(dim=-1, keepdim=True)\n",
        "        \n",
        "        output = torch.zeros_like(x)\n",
        "        \n",
        "        for local_idx, global_expert_idx in enumerate(self.local_expert_indices):\n",
        "            expert_mask = (top_k_indices == global_expert_idx).any(dim=-1)\n",
        "            \n",
        "            if expert_mask.any():\n",
        "                expert_input = x[expert_mask]\n",
        "                expert_probs = top_k_gate_probs[expert_mask]\n",
        "                expert_positions = (top_k_indices[expert_mask] == global_expert_idx).nonzero(as_tuple=True)[1]\n",
        "                \n",
        "                expert_output = self.local_experts[local_idx](expert_input)\n",
        "                expert_weights = expert_probs.gather(1, expert_positions.unsqueeze(1)).squeeze(1)\n",
        "                expert_output = expert_output * expert_weights.unsqueeze(-1)\n",
        "                \n",
        "                output[expert_mask] += expert_output\n",
        "        \n",
        "        dist.all_reduce(output, op=dist.ReduceOp.SUM)\n",
        "        return output\n",
        "\n",
        "# Load base model\n",
        "if rank == 0:\n",
        "    print(f\"Loading Qwen 2.5 3B with 5D parallelism:\")\n",
        "    print(f\"  DP={dp_size}, PP={pp_size}, TP={tp_size}, SP={sp_size}, EP={ep_size}\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=None\n",
        ")\n",
        "\n",
        "# Note: Full MoE integration would require replacing MLP layers with MoE layers\n",
        "# This is a conceptual demonstration of the architecture\n",
        "\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": 1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": {\"enabled\": True},\n",
        "    \"zero_optimization\": {\"stage\": 0},\n",
        "    \"pipeline\": {\"stages\": pp_size},\n",
        "    \"tensor_parallel\": {\"tp_size\": tp_size},\n",
        "    \"wall_clock_breakdown\": True\n",
        "}\n",
        "\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    model_parameters=model.parameters(),\n",
        "    config=deepspeed_config\n",
        ")\n",
        "\n",
        "# Data setup - MetaMathQA using shared utility\n",
        "tokenized_dataset, tokenizer = load_metamathqa_dataset(\n",
        "    split=\"train[:1000]\",\n",
        "    cache_dir=\"./metamathqa_tokenized_data\",\n",
        "    rank=rank\n",
        ")\n",
        "\n",
        "if rank == 0:\n",
        "    print(f\"MetaMathQA dataset loaded: {len(tokenized_dataset)} examples\")\n",
        "\n",
        "MICRO_BATCH_SIZE = 1\n",
        "train_sampler = DistributedSampler(tokenized_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n",
        "train_loader = DataLoader(\n",
        "    tokenized_dataset,\n",
        "    batch_size=MICRO_BATCH_SIZE,\n",
        "    sampler=train_sampler,\n",
        "    collate_fn=lambda x: {\n",
        "        'input_ids': torch.stack([torch.tensor(item['input_ids']) for item in x]),\n",
        "        'attention_mask': torch.stack([torch.tensor(item['attention_mask']) for item in x]),\n",
        "        'labels': torch.stack([torch.tensor(item['labels']) for item in x])\n",
        "    }\n",
        ")\n",
        "\n",
        "# Profiler trace handler\n",
        "def trace_handler(prof):\n",
        "    if rank == 0:\n",
        "        trace_dir = \"./profiler_logs/llm_5d_parallelism_trace\"\n",
        "        os.makedirs(trace_dir, exist_ok=True)\n",
        "        prof.export_chrome_trace(f\"{trace_dir}/rank{rank}_trace.json\")\n",
        "        print(f\"Profiler trace saved to {trace_dir}/rank{rank}_trace.json\")\n",
        "\n",
        "# Training loop\n",
        "model_engine.train()\n",
        "\n",
        "# Profiler schedule: wait=1, warmup=1, active=3 (capture 3 steps), repeat=1\n",
        "with torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
        "    on_trace_ready=trace_handler,\n",
        "    record_shapes=True,\n",
        "    with_stack=True,\n",
        "    profile_memory=True\n",
        ") as prof:\n",
        "    \n",
        "    for epoch in range(1):\n",
        "        train_sampler.set_epoch(epoch)\n",
        "        \n",
        "        for i, batch in enumerate(train_loader):\n",
        "            if i >= 20:\n",
        "                break\n",
        "            \n",
        "            step_start_time = time.time()\n",
        "            \n",
        "            # Label training step\n",
        "            with torch.profiler.record_function(f\"## Training Step {i} ##\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "                \n",
        "                labels[labels == tokenizer.pad_token_id] = -100\n",
        "                \n",
        "                # Label forward pass\n",
        "                with torch.profiler.record_function(\"## Forward Pass ##\"):\n",
        "                    outputs = model_engine(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    loss = outputs.loss\n",
        "                \n",
        "                # Label backward pass\n",
        "                with torch.profiler.record_function(\"## Backward Pass ##\"):\n",
        "                    model_engine.backward(loss)\n",
        "                \n",
        "                # Label optimizer step\n",
        "                with torch.profiler.record_function(\"## Optimizer Step ##\"):\n",
        "                    model_engine.step()\n",
        "            \n",
        "            step_time = time.time() - step_start_time\n",
        "            \n",
        "            # Step profiler (only for first few steps)\n",
        "            if i < 5:  # Profile first 5 steps (wait=1, warmup=1, active=3)\n",
        "                prof.step()\n",
        "        \n",
        "        if rank == 0:\n",
        "            mem_allocated = torch.cuda.memory_allocated() / 1e9\n",
        "            effective_batch = MICRO_BATCH_SIZE * dp_size\n",
        "            seq_len = input_ids.size(1)\n",
        "            throughput_samples = effective_batch / step_time\n",
        "            throughput_tokens = effective_batch * seq_len / step_time\n",
        "            \n",
        "            wandb.log({\n",
        "                \"loss\": loss.item(),\n",
        "                \"step_time_sec\": step_time,\n",
        "                \"memory_allocated_gb\": mem_allocated,\n",
        "                \"throughput_samples_per_sec\": throughput_samples,\n",
        "                \"throughput_tokens_per_sec\": throughput_tokens\n",
        "            })\n",
        "            \n",
        "            print(f\"Step {i}: Loss={loss.item():.4f} | Time={step_time:.2f}s | Mem={mem_allocated:.2f}GB\")\n",
        "\n",
        "if rank == 0:\n",
        "    wandb.finish()\n",
        "dist.destroy_process_group()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run 5D Parallelism experiment on 2 GPUs\n",
        "# !deepspeed --num_gpus=2 train_5d_parallelism_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running Experiments\n",
        "\n",
        "### Test on 2 GPUs First\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all experiments on 2 GPUs with MetaMathQA\n",
        "# Uncomment to run:\n",
        "\n",
        "# 1D Parallelism (DDP)\n",
        "# !torchrun --nproc_per_node=2 train_ddp_qwen3b_metamath.py\n",
        "\n",
        "# 2D Parallelism (Pipeline + Data)\n",
        "# !deepspeed --num_gpus=2 train_2d_pipeline_qwen3b_metamath.py\n",
        "\n",
        "# 3D Parallelism (Data + Pipeline + Tensor)\n",
        "# !deepspeed --num_gpus=2 train_3d_parallelism_qwen3b_metamath.py\n",
        "\n",
        "# 4D Parallelism (Data + Pipeline + Tensor + Sequence)\n",
        "# !deepspeed --num_gpus=2 train_4d_parallelism_qwen3b_metamath.py\n",
        "\n",
        "# 5D Parallelism (Full 5D with MoE)\n",
        "# !deepspeed --num_gpus=2 train_5d_parallelism_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scale to 8 GPUs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all experiments on 8 GPUs with MetaMathQA\n",
        "# Uncomment when you have 8 GPU access:\n",
        "\n",
        "# 1D Parallelism (DDP)\n",
        "# !torchrun --nproc_per_node=8 train_ddp_qwen3b_metamath.py\n",
        "\n",
        "# 2D Parallelism (Pipeline + Data)\n",
        "# !deepspeed --num_gpus=8 train_2d_pipeline_qwen3b_metamath.py\n",
        "\n",
        "# 3D Parallelism (Data + Pipeline + Tensor)\n",
        "# !deepspeed --num_gpus=8 train_3d_parallelism_qwen3b_metamath.py\n",
        "\n",
        "# 4D Parallelism (Data + Pipeline + Tensor + Sequence)\n",
        "# !deepspeed --num_gpus=8 train_4d_parallelism_qwen3b_metamath.py\n",
        "\n",
        "# 5D Parallelism (Full 5D with MoE)\n",
        "# !deepspeed --num_gpus=8 train_5d_parallelism_qwen3b_metamath.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Profiler Trace Analysis\n",
        "\n",
        "### Profiling Setup\n",
        "\n",
        "Each training script includes PyTorch Profiler that:\n",
        "1. **Profiles first 3 training steps** (after 1 warmup step)\n",
        "   - Schedule: `wait=1, warmup=1, active=3, repeat=1`\n",
        "   - Captures detailed traces with memory profiling\n",
        "2. **Saves Chrome traces** to `./profiler_logs/llm_*_trace/` directories\n",
        "3. **Continues full training** after profiling completes\n",
        "\n",
        "**Trace files saved:**\n",
        "- `./profiler_logs/llm_1d_ddp_trace/rank{rank}_trace.json`\n",
        "- `./profiler_logs/llm_2d_pipeline_trace/rank{rank}_trace.json`\n",
        "- `./profiler_logs/llm_3d_parallelism_trace/rank{rank}_trace.json`\n",
        "- `./profiler_logs/llm_4d_parallelism_trace/rank{rank}_trace.json`\n",
        "- `./profiler_logs/llm_5d_parallelism_trace/rank{rank}_trace.json`\n",
        "\n",
        "### Analyze Traces with HTA (Holistic Trace Analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install HTA for trace analysis\n",
        "# !pip install hta\n",
        "\n",
        "# After installing HTA, use this to analyze traces:\n",
        "\"\"\"\n",
        "from hta.trace_analysis import TraceAnalysis\n",
        "import os\n",
        "\n",
        "# Analyze traces for each parallelism strategy\n",
        "trace_dirs = {\n",
        "    \"1D DDP\": \"./profiler_logs/llm_1d_ddp_trace\",\n",
        "    \"2D Pipeline\": \"./profiler_logs/llm_2d_pipeline_trace\",\n",
        "    \"3D Parallelism\": \"./profiler_logs/llm_3d_parallelism_trace\",\n",
        "    \"4D Parallelism\": \"./profiler_logs/llm_4d_parallelism_trace\",\n",
        "    \"5D Parallelism\": \"./profiler_logs/llm_5d_parallelism_trace\"\n",
        "}\n",
        "\n",
        "for name, trace_dir in trace_dirs.items():\n",
        "    if os.path.exists(trace_dir):\n",
        "        print(f\"\\n{'='*20} Analyzing {name} {'='*20}\")\n",
        "        try:\n",
        "            analyzer = TraceAnalysis(trace_dir=trace_dir)\n",
        "            \n",
        "            # Temporal breakdown\n",
        "            temp_df = analyzer.get_temporal_breakdown(visualize=False)\n",
        "            print(f\"\\nTemporal Breakdown:\\n{temp_df}\")\n",
        "            \n",
        "            # Communication vs Computation overlap\n",
        "            overlap_df = analyzer.get_comm_comp_overlap(visualize=False)\n",
        "            print(f\"\\nComm/Comp Overlap:\\n{overlap_df}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Analysis failed: {e}\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"HTA analysis code ready. Uncomment and run after collecting traces.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Analysis and Comparison\n",
        "\n",
        "### Compare 2 GPU vs 8 GPU Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# This cell will analyze results from WandB\n",
        "# After running experiments, fetch results and compare\n",
        "\n",
        "def analyze_scaling_efficiency(results_2gpu, results_8gpu):\n",
        "    \"\"\"\n",
        "    Analyze scaling efficiency from 2 GPUs to 8 GPUs.\n",
        "    \"\"\"\n",
        "    analysis = {}\n",
        "    \n",
        "    for parallelism_type in ['1D', '2D', '3D', '4D', '5D']:\n",
        "        if parallelism_type in results_2gpu and parallelism_type in results_8gpu:\n",
        "            throughput_2gpu = results_2gpu[parallelism_type]['throughput']\n",
        "            throughput_8gpu = results_8gpu[parallelism_type]['throughput']\n",
        "            \n",
        "            ideal_speedup = 4.0  # 8/2\n",
        "            actual_speedup = throughput_8gpu / throughput_2gpu\n",
        "            efficiency = (actual_speedup / ideal_speedup) * 100\n",
        "            \n",
        "            analysis[parallelism_type] = {\n",
        "                'throughput_2gpu': throughput_2gpu,\n",
        "                'throughput_8gpu': throughput_8gpu,\n",
        "                'actual_speedup': actual_speedup,\n",
        "                'ideal_speedup': ideal_speedup,\n",
        "                'efficiency_percent': efficiency\n",
        "            }\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Example usage (after fetching from WandB):\n",
        "# results_2gpu = {'1D': {'throughput': 10}, '2D': {'throughput': 12}, ...}\n",
        "# results_8gpu = {'1D': {'throughput': 35}, '2D': {'throughput': 40}, ...}\n",
        "# scaling_analysis = analyze_scaling_efficiency(results_2gpu, results_8gpu)\n",
        "\n",
        "print(\"Analysis functions ready. Use after running experiments.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates progressive model parallelism strategies with **MetaMathQA** dataset:\n",
        "\n",
        "1. **1D Parallelism**: Data Parallelism (DDP) - Baseline\n",
        "2. **2D Parallelism**: Data + Pipeline Parallelism\n",
        "3. **3D Parallelism**: Data + Pipeline + Tensor Parallelism\n",
        "4. **4D Parallelism**: Data + Pipeline + Tensor + Sequence Parallelism\n",
        "5. **5D Parallelism**: Full 5D with Expert Parallelism (MoE)\n",
        "\n",
        "**Dataset**: MetaMathQA (395K math QA examples) - Industry-standard math reasoning dataset used by MetaMath-Mistral-7B, OpenChat-3.5, CausalLM, and other industry models.\n",
        "\n",
        "**Model**: Qwen 2.5 3B (~3B parameters)\n",
        "\n",
        "All experiments include comprehensive metrics:\n",
        "- Model FLOPs Utilization (MFU)\n",
        "- Memory usage (allocated, reserved, peak)\n",
        "- Throughput (samples/sec, tokens/sec)\n",
        "- Step time breakdown (forward, backward, optimizer)\n",
        "- Communication overhead\n",
        "- Scaling efficiency (2 GPU → 8 GPU)\n",
        "\n",
        "**Next Steps**: Run experiments on 2 GPUs first, then scale to 8 GPUs for full analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results Analysis and Comparison\n",
        "\n",
        "### Compare 2 GPU vs 8 GPU Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# This cell will analyze results from WandB\n",
        "# After running experiments, fetch results and compare\n",
        "\n",
        "def analyze_scaling_efficiency(results_2gpu, results_8gpu):\n",
        "    \"\"\"\n",
        "    Analyze scaling efficiency from 2 GPUs to 8 GPUs.\n",
        "    \"\"\"\n",
        "    analysis = {}\n",
        "    \n",
        "    for parallelism_type in ['1D', '2D', '3D', '4D', '5D']:\n",
        "        if parallelism_type in results_2gpu and parallelism_type in results_8gpu:\n",
        "            throughput_2gpu = results_2gpu[parallelism_type]['throughput']\n",
        "            throughput_8gpu = results_8gpu[parallelism_type]['throughput']\n",
        "            \n",
        "            ideal_speedup = 4.0  # 8/2\n",
        "            actual_speedup = throughput_8gpu / throughput_2gpu\n",
        "            efficiency = (actual_speedup / ideal_speedup) * 100\n",
        "            \n",
        "            analysis[parallelism_type] = {\n",
        "                'throughput_2gpu': throughput_2gpu,\n",
        "                'throughput_8gpu': throughput_8gpu,\n",
        "                'actual_speedup': actual_speedup,\n",
        "                'ideal_speedup': ideal_speedup,\n",
        "                'efficiency_percent': efficiency\n",
        "            }\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Example usage (after fetching from WandB):\n",
        "# results_2gpu = {'1D': {'throughput': 10}, '2D': {'throughput': 12}, ...}\n",
        "# results_8gpu = {'1D': {'throughput': 35}, '2D': {'throughput': 40}, ...}\n",
        "# scaling_analysis = analyze_scaling_efficiency(results_2gpu, results_8gpu)\n",
        "\n",
        "print(\"Analysis functions ready. Use after running experiments.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
