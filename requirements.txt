# Core PyTorch and Deep Learning
torch>=2.0.0
torchvision>=0.15.0
torchaudio>=2.0.0

# DeepSpeed for FSDP and distributed training
deepspeed>=0.10.0

# Hugging Face Transformers and Datasets
transformers>=4.30.0
datasets>=2.12.0
accelerate>=0.20.0
peft>=0.5.0  # For LoRA (Low-Rank Adaptation) in single-GPU experiments

# LLM-Specific Dependencies
sentencepiece>=0.1.99  # Required for Qwen 2.5 and other tokenizers (LLM Model Parallelism notebook)
protobuf>=3.20.0  # Required for transformers and dataset serialization (MetaMathQA dataset)

# Computer Vision and Image Processing
Pillow>=9.5.0
opencv-python>=4.8.0
albumentations>=1.3.0

# Data Science and Analysis
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
scipy>=1.10.0  # Required for HTA (Holistic Trace Analysis)

# Visualization and Plotting
plotly>=5.15.0
wandb>=0.15.0

# Configuration and Utilities
pyyaml>=6.0
tqdm>=4.65.0
tensorboard>=2.13.0

# Jupyter and Development
jupyter>=1.0.0
ipykernel>=6.23.0
notebook>=6.5.0

# Additional ML Libraries
einops>=0.6.0
timm>=0.9.0

# System and Performance
psutil>=5.9.0
GPUtil>=1.4.0

# Distributed Training Support
# Note: NCCL is typically provided by the HPC cluster
# apex  # Optional for NVIDIA Apex (if needed)
mpi4py>=3.1.0  # Required for multi-GPU distributed training with DeepSpeed

# Performance Analysis and Profiling
hta>=0.1.0  # Holistic Trace Analysis for PyTorch profiling traces
# Alternative: HolisticTraceAnalysis  # Can use this package name instead

# Testing and Quality
pytest>=7.4.0
black>=23.0.0
flake8>=6.0.0
