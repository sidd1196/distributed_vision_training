{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Adaptation and Analysis of Vision Transformers using LoRA"
      ],
      "metadata": {
        "id": "ewAZzlObaczz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"=== PyTorch Environment Test ===\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # Test GPU tensor\n",
        "    x = torch.randn(3, 3).cuda()\n",
        "    print(f\"\\nGPU tensor created: {x.device}\")\n",
        "    print(f\"Tensor shape: {x.shape}\")\n",
        "else:\n",
        "    print(\"CUDA not available - using CPU\")\n",
        "    x = torch.randn(3, 3)\n",
        "    print(f\"CPU tensor created: {x.device}\")\n",
        "\n",
        "print(\"\\n✅ PyTorch test completed!\")"
      ],
      "metadata": {
        "id": "OWw0nugHYljv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00373a7-c171-47a8-ffc8-5ba96d277cff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== PyTorch Environment Test ===\n",
            "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "Number of GPUs: 1\n",
            "GPU name: Tesla T4\n",
            "\n",
            "GPU tensor created: cuda:0\n",
            "Tensor shape: torch.Size([3, 3])\n",
            "\n",
            "✅ PyTorch test completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downloading the data"
      ],
      "metadata": {
        "id": "17dCZxUzMhMD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "770b94b9",
        "outputId": "cd9a0565-4d29-4162-81af-3870314400a8"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the CIFAR-100 training dataset\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "# Load the CIFAR-100 test dataset\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "print(\"CIFAR-100 dataset imported successfully.\")\n",
        "print(f\"Training set size: {len(trainset)}\")\n",
        "print(f\"Test set size: {len(testset)}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-100 dataset imported successfully.\n",
            "Training set size: 50000\n",
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VZywhoZLYll_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Resizing the data for ViT model\n"
      ],
      "metadata": {
        "id": "vmiQ12PxMvRG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6edb525",
        "outputId": "aba4837f-ed1c-4dc3-ee01-c87e420c4367"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define transforms for training and validation/testing\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Upsample to ViT resolution\n",
        "    transforms.RandomHorizontalFlip(), # Example data augmentation\n",
        "    transforms.RandomCrop(224, padding=4), # Example data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # Upsample to ViT resolution\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Apply the transforms to the datasets\n",
        "trainset.transform = train_transform\n",
        "testset.transform = test_transform\n",
        "\n",
        "print(\"Data preparation complete. Transforms applied to datasets.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preparation complete. Transforms applied to datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceO7JH5qYloQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the VIT and freezing the parameters"
      ],
      "metadata": {
        "id": "QGUdLT2ZM1H7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42daad35",
        "outputId": "ed9936fd-21d6-4c8e-9951-b8afe60a89f3"
      },
      "source": [
        "from transformers import ViTForImageClassification\n",
        "\n",
        "# Load a pre-trained Vision Transformer model\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=100, ignore_mismatched_sizes=True)\n",
        "\n",
        "# Freeze all parameters\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False # This freezes the parameters\n",
        "\n",
        "print(\"Pre-trained ViT model loaded and parameters frozen.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained ViT model loaded and parameters frozen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qd01EMRNYlqP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e486f47b"
      },
      "source": [
        "# %pip install peft transformers datasets"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "621f2f8d",
        "outputId": "07bfbaf9-9963-4d47-c705-d2d21d9037bd"
      },
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Define LoRA configuration\n",
        "config = LoraConfig(\n",
        "    r=16, # Rank of the update matrices.\n",
        "    lora_alpha=16, # Scaling factor for the LoRA update.\n",
        "    target_modules=[\"query\", \"value\"], # Modules to apply LoRA to.\n",
        "    lora_dropout=0.1, # Dropout probability for LoRA layers.\n",
        "    bias=\"none\", # Bias type.\n",
        ")\n",
        "\n",
        "# Get the LoRA-infused model\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# Print trainable parameters\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"LoRA adapters integrated into the model.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 589,824 || all params: 86,465,380 || trainable%: 0.6822\n",
            "LoRA adapters integrated into the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17a23fae"
      },
      "source": [
        "# You need to reinstall DeepSpeed and force it to compile this special CPU Adam extension.\n",
        "\n",
        "# # # Uninstall the old version first\n",
        "# !pip uninstall deepspeed -y\n",
        "\n",
        "# # Re-install with the build flag for CPUAdam\n",
        "# !DS_BUILD_CPU_ADAM=1 pip install deepspeed\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqQBFTAwqlTV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "459dc0d2"
      },
      "source": [
        "Next, we need to create a DeepSpeed configuration file. This is typically a JSON file that specifies the various optimization settings for DeepSpeed. Here's an example configuration for mixed precision training and ZeRO Stage 2 optimization, which is often used for memory efficiency.\n",
        "\n",
        "You can save this configuration to a file named `deepspeed_config.json`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ae815af",
        "outputId": "57bb2f7c-759f-40f4-e15a-ecbd33a88a7f"
      },
      "source": [
        "%%writefile deepspeed_config.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000\n",
        "    },\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        }\n",
        "    },\n",
        "    \"zero_force_ds_cpu_optimizer\": false,\n",
        "    \"train_batch_size\": 16,\n",
        "    \"train_micro_batch_size_per_gpu\": 16,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"steps_per_print\": 200\n",
        "}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting deepspeed_config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Create the DataLoaders"
      ],
      "metadata": {
        "id": "X0Gsft8sl6yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(testset, batch_size=16, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders created.\")"
      ],
      "metadata": {
        "id": "ht7eS4piYl13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edb4a52-7576-4934-86c4-5ba07207d238"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Enable Gradient Checkpointing"
      ],
      "metadata": {
        "id": "MIEb8JphmELT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "print(\"Gradient checkpointing enabled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV14uvsLmASg",
        "outputId": "4b73a9b9-4d25-47c8-c306-67123fac1738"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient checkpointing enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Initialize DeepSpeed"
      ],
      "metadata": {
        "id": "NFW9w8YcmQBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install mpi4py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ZDhNyPmaMs",
        "outputId": "ed694194-05ca-4751-9338-b650af4807f2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.12/dist-packages (4.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import deepspeed\n",
        "\n",
        "# 1. Manually create the standard PyTorch optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# 2. Initialize DeepSpeed, passing the optimizer you just created\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    optimizer=optimizer,  # Pass the optimizer here\n",
        "    config_params='deepspeed_config.json'\n",
        ")\n",
        "\n",
        "print(\"DeepSpeed engine initialized with PyTorch AdamW (forced).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UnTnzyjmBBJ",
        "outputId": "cd72f5f2-b1ef-4e0b-aa4f-75a394ee94da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSpeed engine initialized with PyTorch AdamW (forced).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Write the Training Script"
      ],
      "metadata": {
        "id": "LBsbc_AqS7G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTForImageClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import deepspeed\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"--- Initializing Training Script ---\")\n",
        "\n",
        "# --- 1. Data Prep ---\n",
        "print(\"Setting up data transformations...\")\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "print(\"Loading CIFAR-100 dataset...\")\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)\n",
        "print(\"DataLoaders created.\")\n",
        "\n",
        "# --- 2. Model Setup ---\n",
        "print(\"Loading pre-trained ViT model...\")\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=100, ignore_mismatched_sizes=True)\n",
        "\n",
        "# Freeze all parameters first\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# --- 3. LoRA Setup ---\n",
        "print(\"Applying LoRA adapters...\")\n",
        "config = LoraConfig(\n",
        "    r=16, lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1, bias=\"none\",\n",
        ")\n",
        "model = get_peft_model(model, config)\n",
        "print(\"LoRA adapters applied.\")\n",
        "\n",
        "# --- !! CORRECTED ORDER: Unfreeze classifier AFTER LoRA !! ---\n",
        "print(\"Unfreezing classification head...\")\n",
        "for param in model.classifier.parameters():\n",
        "    param.requires_grad = True\n",
        "# --- End of Fix ---\n",
        "\n",
        "print(\"New trainable parameters:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# --- 4. Gradient Checkpointing (Task 8) ---\n",
        "print(\"Enabling gradient checkpointing...\")\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# --- 5. DeepSpeed Initialization (Task 8) ---\n",
        "print(\"Initializing DeepSpeed...\")\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    config_params='deepspeed_config.json'\n",
        ")\n",
        "print(\"DeepSpeed engine initialized successfully.\")\n",
        "\n",
        "# --- 6. Training Loop (Task 9) ---\n",
        "device = model_engine.device\n",
        "num_epochs = 3 # Start with 3-5 epochs to test\n",
        "\n",
        "print(f\"--- Starting training for {num_epochs} epochs ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model_engine.train()\n",
        "    total_loss = 0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_engine(inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        model_engine.backward(loss)\n",
        "        model_engine.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if i % 100 == 0:\n",
        "            print(f\"  Epoch {epoch+1}, Step {i}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"**Epoch {epoch+1}/{num_epochs} - Avg. Training Loss: {total_loss / len(train_loader):.4f}**\")\n",
        "\n",
        "print(\"--- Training complete ---\")\n",
        "\n",
        "# --- 7. Evaluation (Task 11) ---\n",
        "print(\"--- Starting evaluation ---\")\n",
        "model_engine.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, labels = batch\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model_engine(inputs)\n",
        "\n",
        "        _, predicted = torch.max(outputs.logits.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"**Final Test Accuracy: {accuracy:.2f}%**\")\n",
        "\n",
        "# --- 8. Confusion Matrix (Task 11) ---\n",
        "print(\"Generating confusion matrix...\")\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "print(\"Confusion Matrix (first 10x10):\")\n",
        "print(cm[:10, :10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgSVKeKMmBDT",
        "outputId": "290f62b1-9bf9-4f28-c177-f3f2eeb3b42b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install libraries for the script, just in case\n",
        "!pip install deepspeed scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H5AFJ0pmBGx",
        "outputId": "85674c93-2385-4832-db84-710ee8fc59ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deepspeed in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from deepspeed) (0.8.1)\n",
            "Requirement already satisfied: hjson in /usr/local/lib/python3.12/dist-packages (from deepspeed) (3.1.0)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.1.2)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from deepspeed) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from deepspeed) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from deepspeed) (9.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.11.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from deepspeed) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from deepspeed) (4.67.1)\n",
            "Requirement already satisfied: nvidia-ml-py in /usr/local/lib/python3.12/dist-packages (from deepspeed) (12.575.51)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->deepspeed) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->deepspeed) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->deepspeed) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch the training script with DeepSpeed\n",
        "# !deepspeed --num_gpus=1 train.py\n",
        "# We're just adding --master_port 29501 to pick a new, free port\n",
        "!deepspeed --num_gpus=1 --master_port 29501 train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD3rJx22mBIp",
        "outputId": "f0f3b532-f8aa-4ed8-fb78-a2fe98a0815d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-10-19 21:49:20.411098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760910560.431753   30106 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760910560.437907   30106 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760910560.453609   30106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910560.453637   30106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910560.453640   30106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910560.453645   30106 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[2025-10-19 21:49:23,566] [WARNING] [runner.py:232:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2025-10-19 21:49:23,566] [INFO] [runner.py:630:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29501 --enable_each_rank_log=None --log_level=info train.py\n",
            "2025-10-19 21:49:31.365379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760910571.385694   30232 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760910571.391895   30232 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760910571.407359   30232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910571.407384   30232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910571.407387   30232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910571.407389   30232 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "[2025-10-19 21:49:34,504] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.22.3-1+cuda12.5\n",
            "[2025-10-19 21:49:34,504] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-10-19 21:49:34,504] [INFO] [launch.py:155:main] 0 NCCL_VERSION=2.22.3-1\n",
            "[2025-10-19 21:49:34,504] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
            "[2025-10-19 21:49:34,504] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE=libnccl2=2.22.3-1+cuda12.5\n",
            "[2025-10-19 21:49:34,504] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:155:main] 0 NV_LIBNCCL_PACKAGE_VERSION=2.22.3-1\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:162:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:168:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:179:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:180:main] dist_world_size=1\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:184:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2025-10-19 21:49:34,505] [INFO] [launch.py:272:main] process 30362 spawned with command: ['/usr/bin/python3', '-u', 'train.py', '--local_rank=0']\n",
            "2025-10-19 21:49:39.947779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760910579.967788   30362 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760910579.974113   30362 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760910579.988957   30362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910579.988982   30362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910579.988985   30362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760910579.988987   30362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "--- Initializing Training Script ---\n",
            "Setting up data transformations...\n",
            "Loading CIFAR-100 dataset...\n",
            "DataLoaders created.\n",
            "Loading pre-trained ViT model...\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Applying LoRA adapters...\n",
            "LoRA adapters applied.\n",
            "Unfreezing classification head...\n",
            "New trainable parameters:\n",
            "trainable params: 666,724 || all params: 86,465,380 || trainable%: 0.7711\n",
            "Enabling gradient checkpointing...\n",
            "Initializing DeepSpeed...\n",
            "DeepSpeed engine initialized successfully.\n",
            "--- Starting training for 3 epochs ---\n",
            "  Epoch 1, Step 0: Loss = 4.7760\n",
            "  Epoch 1, Step 100: Loss = 4.6487\n",
            "  Epoch 1, Step 200: Loss = 4.1170\n",
            "  Epoch 1, Step 300: Loss = 3.8681\n",
            "  Epoch 1, Step 400: Loss = 3.4364\n",
            "  Epoch 1, Step 500: Loss = 3.3963\n",
            "  Epoch 1, Step 600: Loss = 2.7568\n",
            "  Epoch 1, Step 700: Loss = 2.5770\n",
            "  Epoch 1, Step 800: Loss = 2.6748\n",
            "  Epoch 1, Step 900: Loss = 2.2211\n",
            "  Epoch 1, Step 1000: Loss = 1.9621\n",
            "  Epoch 1, Step 1100: Loss = 1.7326\n",
            "  Epoch 1, Step 1200: Loss = 1.6982\n",
            "  Epoch 1, Step 1300: Loss = 1.4745\n",
            "  Epoch 1, Step 1400: Loss = 1.0852\n",
            "  Epoch 1, Step 1500: Loss = 1.0983\n",
            "  Epoch 1, Step 1600: Loss = 1.3855\n",
            "  Epoch 1, Step 1700: Loss = 1.0558\n",
            "  Epoch 1, Step 1800: Loss = 1.6435\n",
            "  Epoch 1, Step 1900: Loss = 1.0676\n",
            "  Epoch 1, Step 2000: Loss = 1.0576\n",
            "  Epoch 1, Step 2100: Loss = 0.6151\n",
            "  Epoch 1, Step 2200: Loss = 0.8183\n",
            "  Epoch 1, Step 2300: Loss = 0.8573\n",
            "  Epoch 1, Step 2400: Loss = 0.8400\n",
            "  Epoch 1, Step 2500: Loss = 1.1726\n",
            "  Epoch 1, Step 2600: Loss = 0.6406\n",
            "  Epoch 1, Step 2700: Loss = 0.7530\n",
            "  Epoch 1, Step 2800: Loss = 0.3566\n",
            "  Epoch 1, Step 2900: Loss = 0.5847\n",
            "  Epoch 1, Step 3000: Loss = 1.1135\n",
            "  Epoch 1, Step 3100: Loss = 0.7675\n",
            "**Epoch 1/3 - Avg. Training Loss: 1.7956**\n",
            "  Epoch 2, Step 0: Loss = 1.1615\n",
            "  Epoch 2, Step 100: Loss = 0.5913\n",
            "  Epoch 2, Step 200: Loss = 0.6410\n",
            "  Epoch 2, Step 300: Loss = 0.5394\n",
            "  Epoch 2, Step 400: Loss = 0.7000\n",
            "  Epoch 2, Step 500: Loss = 1.0442\n",
            "  Epoch 2, Step 600: Loss = 0.7305\n",
            "  Epoch 2, Step 700: Loss = 0.6771\n",
            "  Epoch 2, Step 800: Loss = 0.5872\n",
            "  Epoch 2, Step 900: Loss = 0.9725\n",
            "  Epoch 2, Step 1000: Loss = 0.4551\n",
            "  Epoch 2, Step 1100: Loss = 1.0409\n",
            "  Epoch 2, Step 1200: Loss = 0.7324\n",
            "  Epoch 2, Step 1300: Loss = 0.6776\n",
            "  Epoch 2, Step 1400: Loss = 0.8033\n",
            "  Epoch 2, Step 1500: Loss = 0.8401\n",
            "  Epoch 2, Step 1600: Loss = 0.3057\n",
            "  Epoch 2, Step 1700: Loss = 1.0300\n",
            "  Epoch 2, Step 1800: Loss = 0.6571\n",
            "  Epoch 2, Step 1900: Loss = 0.9212\n",
            "  Epoch 2, Step 2000: Loss = 0.5953\n",
            "  Epoch 2, Step 2100: Loss = 0.9146\n",
            "  Epoch 2, Step 2200: Loss = 0.6818\n",
            "  Epoch 2, Step 2300: Loss = 0.5692\n",
            "  Epoch 2, Step 2400: Loss = 0.7974\n",
            "  Epoch 2, Step 2500: Loss = 0.3085\n",
            "  Epoch 2, Step 2600: Loss = 0.6862\n",
            "  Epoch 2, Step 2700: Loss = 0.5742\n",
            "  Epoch 2, Step 2800: Loss = 0.5737\n",
            "  Epoch 2, Step 2900: Loss = 0.7371\n",
            "  Epoch 2, Step 3000: Loss = 0.5124\n",
            "  Epoch 2, Step 3100: Loss = 0.6403\n",
            "**Epoch 2/3 - Avg. Training Loss: 0.6614**\n",
            "  Epoch 3, Step 0: Loss = 0.2031\n",
            "  Epoch 3, Step 100: Loss = 0.6702\n",
            "  Epoch 3, Step 200: Loss = 0.7857\n",
            "  Epoch 3, Step 300: Loss = 0.4125\n",
            "  Epoch 3, Step 400: Loss = 1.0536\n",
            "  Epoch 3, Step 500: Loss = 0.5735\n",
            "  Epoch 3, Step 600: Loss = 0.4992\n",
            "  Epoch 3, Step 700: Loss = 0.8313\n",
            "  Epoch 3, Step 800: Loss = 0.4513\n",
            "  Epoch 3, Step 900: Loss = 0.2370\n",
            "  Epoch 3, Step 1000: Loss = 0.5114\n",
            "  Epoch 3, Step 1100: Loss = 0.3836\n",
            "  Epoch 3, Step 1200: Loss = 0.6449\n",
            "  Epoch 3, Step 1300: Loss = 0.3647\n",
            "  Epoch 3, Step 1400: Loss = 0.3340\n",
            "  Epoch 3, Step 1500: Loss = 0.3677\n",
            "  Epoch 3, Step 1600: Loss = 0.3863\n",
            "  Epoch 3, Step 1700: Loss = 0.1751\n",
            "  Epoch 3, Step 1800: Loss = 0.5456\n",
            "  Epoch 3, Step 1900: Loss = 0.4791\n",
            "  Epoch 3, Step 2000: Loss = 0.5669\n",
            "  Epoch 3, Step 2100: Loss = 1.1332\n",
            "  Epoch 3, Step 2200: Loss = 0.6112\n",
            "  Epoch 3, Step 2300: Loss = 0.1145\n",
            "  Epoch 3, Step 2400: Loss = 0.8721\n",
            "  Epoch 3, Step 2500: Loss = 0.5945\n",
            "  Epoch 3, Step 2600: Loss = 0.6602\n",
            "  Epoch 3, Step 2700: Loss = 0.4623\n",
            "  Epoch 3, Step 2800: Loss = 0.1525\n",
            "  Epoch 3, Step 2900: Loss = 0.4283\n",
            "  Epoch 3, Step 3000: Loss = 0.6374\n",
            "  Epoch 3, Step 3100: Loss = 0.2772\n",
            "**Epoch 3/3 - Avg. Training Loss: 0.5476**\n",
            "--- Training complete ---\n",
            "--- Starting evaluation ---\n",
            "**Final Test Accuracy: 83.00%**\n",
            "Generating confusion matrix...\n",
            "Confusion Matrix (first 10x10):\n",
            "[[95  0  0  0  0  1  0  0  0  0]\n",
            " [ 0 90  0  1  0  0  0  0  0  0]\n",
            " [ 0  0 74  0  0  3  0  0  0  0]\n",
            " [ 0  0  0 83  2  0  0  0  0  0]\n",
            " [ 0  0  0  1 75  0  0  0  0  0]\n",
            " [ 0  0  0  0  0 96  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 90  0  0  0]\n",
            " [ 0  0  0  0  0  0  2 89  0  0]\n",
            " [ 0  0  0  0  0  0  0  0 91  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 91]]\n",
            "[rank0]:[W1019 22:22:22.877211702 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "[2025-10-19 22:22:24,797] [INFO] [launch.py:367:main] Process 30362 exits successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zzHisU9Zav59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}